<!--<!--
Title: "fouille_theorie_math"
Author: rsquaredata
Last updated: 2025-12-09
-->

# Fouille de donn√©es massives - Th√©orie & Math√©matiques  
---

## Table des mati√®res

1. [Notions g√©n√©rales](#1-notions-g√©n√©rales)
2. [Apprentissage supervis√© vs non-supervis√©](#2-apprentissage-supervis√©-vs-non-supervis√©)
3. [Surrogate loss & optimisation](#3-surrogate-loss--optimisation)
4. [SVM lin√©aire](#4-svm-lin√©aire)
5. [Probl√®me dual & vecteurs de support](#5-probl√®me-dual--vecteurs-de-support)
6. [SVM √† noyau - Kernel Trick](#6-svm-√†-noyau---kernel-trick)
7. [M√©thodes √† noyaux - types et propri√©t√©s](#7-m√©thodes-√†-noyaux---types-et-propri√©t√©s)
8. [Boosting et Gradient Boosting](#8-boosting-et-gradient-boosting)
9. [Big Data & Machine Learning scalable](#9-big-data--machine-learning-scalable)
10. [R√©sum√© formules cl√©s](#10-r√©sum√©-formules-cl√©s)

---

## 1. Notions g√©n√©rales

| Terme | D√©finition courte | Formule / Interpr√©tation |
|-------|--------------------|--------------------------|
| **Observation** | Instance de donn√©es, not√©e $x_i \in \mathbb{R}^d$ | vecteur ligne |
| **Feature (variable explicative)** | composante de $x_i$ | $x_i = [x_{i1}, \dots, x_{id}]$ |
| **Cible** | valeur √† pr√©dire | $y_i \in \{-1, +1\}$ (classif) ou $y_i \in \mathbb{R}$ (r√©gr.) |
| **Mod√®le** | fonction $f : X \to Y$ estim√©e sur un √©chantillon | $f(x) = \hat{y}$ |
| **Hypoth√®se** | forme param√©trique de $f$ | ex. lin√©aire : $f(x) = w^\top x + b$ |
| **Risque empirique** | erreur moyenne sur l‚Äô√©chantillon | $R(f) = \frac{1}{n}\sum_i L(y_i, f(x_i))$ |
| **Risque attendu** | esp√©rance du risque sur la vraie distribution | $E_{(x,y)}[L(y,f(x))]$ |
| **Biais / Variance** | d√©composition de l‚Äôerreur | Erreur = Biais¬≤ + Variance + bruit irr√©ductible |

---

## 2. Apprentissage supervis√© vs non-supervis√©

| Type | Objectif | Exemples |
|------|-----------|----------|
| **Supervis√©** | pr√©dire $y$ √† partir de $x$ | r√©gression, classification |
| **Non-supervis√©** | explorer la structure de $X$ sans $y$ | clustering, ACP, d√©tection anomalies |
| **Semi-supervis√©** | partiellement √©tiquet√© | pseudo-labelling, auto-encoder |
| **Auto-supervis√©** | labels g√©n√©r√©s √† partir des donn√©es | masquage (BERT), contrastif |
| **Renforcement** | apprendre une politique via r√©compense | RL, bandits multi-bras |

---

## 3. Surrogate loss & optimisation

### 3.1. Id√©e cl√©
- La **0-1 loss** ($L(y,f(x)) = \mathbb{1}_{(yf(x) \lt 0)}$) n‚Äôest pas d√©rivable ‚Üí on la **remplace** par une *surrogate loss* convexe.  
- Chaque *surrogate loss* conduit √† un **mod√®le diff√©rent**.

| T√¢che | Loss id√©ale | Surrogate courante | Algorithme | Expression |
|-------|--------------|--------------------|-------------|-------------|
| Classif. binaire | 0-1 loss | Hinge loss | **SVM** | $\max(0, 1 - y f(x))$ |
| Classif. proba | 0-1 loss | Log loss | **R√©gr. logistique, Boosting** | $\log(1+e^{-y f(x)})$ |
| AdaBoost | 0-1 loss | Exponential loss | **AdaBoost** | $e^{-y f(x)}$ |
| R√©gression | L‚ÇÇ | MSE | R√©gr. lin√©aire / Ridge | $(y - f(x))^2$ |
| R√©gression robuste | L‚ÇÅ | MAE | Lasso | $\lvert y - f(x) \rvert$ |

### 3.2. Fonction de co√ªt globale

$$
J(w) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \lambda \, \|w\|^2
$$

- Premier terme : **ajustement** au jeu d‚Äôapprentissage  
- Deuxi√®me terme : **r√©gularisation** (contr√¥le de la complexit√©)

---

## 4. SVM lin√©aire

### 4.1. Principe
Trouver un **hyperplan s√©parateur** de marge maximale entre deux classes.

$$
\min_{w,b} \frac{1}{2}\|w\|^2 \quad \text{s.c.} \quad y_i(w^\top x_i + b) \ge 1
$$

### 4.2. Cas non s√©parable
On autorise des erreurs via les **variables d‚Äô√©cart** $\xi_i$ :

$$
\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_i \xi_i
\quad \text{s.c. } y_i(w^\top x_i + b) \ge 1 - \xi_i, \, \xi_i \ge 0
$$

- $C$ : param√®tre de p√©nalisation (grande $C$ ‚Üí peu d‚Äôerreurs mais surfit)
- La **marge** vaut $\frac{2}{ \vert w \vert }$.

### 4.3. Interpr√©tation g√©om√©trique
- Hyperplan : $w^\top x + b = 0$
- Classes : $\text{sign}(w^\top x + b)$
- Vecteurs supports : points sur la marge $y_i(w^\top x_i + b) = 1$

---

## 5. Probl√®me dual & vecteurs de support

### 5.1. Construction du dual (cas lin√©aire s√©parable)
On introduit les multiplicateurs de Lagrange $\alpha_i \ge 0$.

$$
L(w,b,\alpha) = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i[y_i(w^\top x_i + b) - 1]
$$

Conditions :

$$
\begin{cases}
\nabla_w L = 0 \Rightarrow w = \sum_i \alpha_i y_i x_i \\
\nabla_b L = 0 \Rightarrow \sum_i \alpha_i y_i = 0
\end{cases}
$$

‚Üí on obtient le **probl√®me dual** :

$$
\max_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j (x_i^\top x_j)
\quad \text{s.c. } \alpha_i \ge 0, \; \sum_i \alpha_i y_i = 0
$$

### 5.2. D√©cision finale

$$
f(x) = \text{sign}\left(\sum_i \alpha_i y_i (x_i^\top x) + b \right)
$$

Seuls les points avec $\alpha_i > 0$ influencent $f$ ‚Üí ce sont les **vecteurs de support**.

---

## 6. SVM √† noyau - *Kernel Trick*

### 6.1. Motivation
Quand les donn√©es ne sont **pas s√©parables lin√©airement**, on projette $x$ dans un **espace de plus grande dimension** $\phi(x)$.

Mais au lieu de calculer $\phi(x)$ explicitement, on utilise une **fonction noyau** :

$$
K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
$$

### 6.2. Dual √† noyau

$$
\max_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j K(x_i, x_j)
$$

et  

$$
f(x) = \text{sign}\left(\sum_i \alpha_i y_i K(x_i,x) + b\right)
$$

### 6.3. Conditions de validit√© d‚Äôun noyau
- Sym√©trie : $K(x_i, x_j) = K(x_j, x_i)$  
- Matrice de Gram $[K(x_i,x_j)]$ semi-d√©finie positive (crit√®re de Mercer)

<details><summary><span style="color:lightblue">Matrice semi-d√©finie positive</span></summary>

**ELI5** :
- Imaginons une matrice $K$ qui contient toutes les ‚Äúsimilarit√©s‚Äù entre les points : $K_{ij} = K(x_i, x_j)$
  - Si deux points sont tr√®s proches ‚Üí grande valeur
  - Si deux points sont diff√©rents ‚Üí petite valeur
- On veut que cette matrice soit "coh√©rente", c‚Äôest-√†-dire qu‚Äôelle ne puisse pas donner des distances n√©gatives ni des comportements absurdes (comme "la similarit√© de moi avec moi est n√©gative").
-  matrice **semi-d√©finie positive** garantit √ßa : **aucune combinaison lin√©aire de lignes/colonnes ne produit une valeur n√©gative**.
- En image : l‚Äôespace d√©fini par le noyau **ne courbe pas l‚Äôespace dans un sens "non physique"** ; "l‚Äô√©nergie associ√©e √† toute combinaison de points est toujours positive ou nulle".

**D√©finition math√©matique** :
Une matrice $K \in \mathbb{R}^{n \times n}$ est dite **semi-d√©finie positive (SDP)** si : $\forall v \in \mathbb{R}^\top, \quad  v^\top Kv \ge 0$.  
Autrement dit :
- si on prend n‚Äôimporte quel vecteur $v$ (de m√™me taille que $K$),
- qu'on multiplie $v^\top K v$ (c‚Äôest un produit scalaire pond√©r√©),
- le r√©sultat doit **toujours √™tre positif ou nul**.

**Interpr√©taion g√©om√©trique** :
- Une matrice SDP repr√©sente une **forme quadratique** qui **ne change pas le signe** du vecteur.
- Si elle est strictement positive ($v^\top K v > 0$ pour tout $v \ne 0$), elle est **d√©finie positive**.
- Si elle peut valoir 0 pour certains $v \ne 0$, elle est **semi-d√©finie positive**.

**Application au SVM et aux noyaux** :
- Dans un SVM √† noyau, on remplace le produit scalaire $x_i^\top x_j$ par $K(x_i,x_j)$, ce qui revient √† construire une matrice noyau :  

$$
K = \begin{pmatrix}
K{x_1,x_1} & K{x_1,x_2} & \ldots & K{x_1,x_n} \\
K{x_2,x_1} & K{x_2,x_2} & \ldots & K{x_2,x_n} \\
\vdots &  & \ddots & \vdots \\
K{x_n,x_1} & K{x_n,x_2} & \ldots & K{x_n,x_n}
\end{pmatrix}
$$

- Pour que le noyau soit valide, il faut que cette matrice soit **sym√©trique et semi-d√©finie positive** : c‚Äôest la condition de Mercer. Sinon, le SVM pourrait se comporter de mani√®re instable (marge n√©gative, √©nergie impossible, etc.).

**Comment v√©rifier q'une matrice est SDP** :  
1. **Via les valeurs propres** : $K$ est SDP \Leftrightarrow toutes ses valeurs propres $\lambda_i \ge 0$.

```python
import numpy as np

K = np.array([[2, -1], [-1, 2]])

eigvals = np.linalg.eigvals(K)
print(eigvals)   # [3. 1.] ‚Üí toutes positives ‚áí SDP
```

2. **Via la condition $v^\top Kv \ge 0$** :

```python
v = np.random.randn(K.shape[0])
print(v.T @ K @ v)  # doit √™tre ‚â• 0
```

3. **Via `np.allclose` sur la sym√©trie** :

```python
np.allclose(K, K.T)   # doit √™tre True
```

</details>

---

## 7. M√©thodes √† noyaux - types et propri√©t√©s

| Type | Expression | Param√®tres | Usage typique |
|-------|-------------|-------------|---------------|
| **Lin√©aire** | $K(x,x') = x^\top x'$ | ‚Äì | donn√©es lin√©aires |
| **Polynomial** | $(x^\top x' + c)^d$ | degr√© $d$, cste $c$ | interactions polynomiales |
| **RBF / Gaussien** | $\exp(-\gamma \|x - x'\|^2)$ | $\gamma$ (largeur du noyau) | fronti√®res lisses non lin√©aires |
| **Sigmo√Øde** | $\tanh(\kappa x^\top x' + c)$ | $\kappa, c$ | inspiration r√©seaux de neurones |
| **DTW / string / graph kernels** | sp√©cifiques au domaine | - | s√©ries, textes, graphes |

‚Üí Choix du noyau = compromis entre **flexibilit√©** et **surapprentissage**.

---

## 8. Boosting et Gradient Boosting

### 8.1. AdaBoost (Adaptive Boosting)
Principe : combiner plusieurs *weak learners* (arbres peu profonds) entra√Æn√©s s√©quentiellement.

1. Initialiser des poids $w_i = 1/n$  
2. Pour chaque it√©ration $t$ :
   - entra√Æner $h_t(x)$ sur les donn√©es pond√©r√©es  
   - calculer l‚Äôerreur pond√©r√©e $\varepsilon_t = \sum_i w_i \mathbb{1}(h_t(x_i) \ne y_i)$  
   - pond√©rer le mod√®le : $\alpha_t = \frac{1}{2}\log\frac{1-\varepsilon_t}{\varepsilon_t}$  
   - mettre √† jour les poids : $w_i \leftarrow w_i e^{-\alpha_t y_i h_t(x_i)}$ puis normaliser  

Pr√©diction finale :

$$
F(x) = \text{sign}\left(\sum_t \alpha_t h_t(x)\right)
$$

‚Üí Maximisation de la **marge moyenne**, am√©liore la robustesse.

---

### 8.2. Gradient Boosting

- Vue comme une **descente de gradient fonctionnelle** :
  chaque nouveau mod√®le apprend √† **corriger les r√©sidus** du pr√©c√©dent.

$$
F_0(x) = \arg\min_\gamma \sum_i L(y_i, \gamma)
$$

$$
r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F=F_{m-1}}
$$

$$
F_m(x) = F_{m-1}(x) + \nu \, h_m(x)
$$

o√π $\nu$ = *learning rate*.

| Variante | Particularit√©s |
|-----------|----------------|
| **XGBoost** | r√©gularisation L‚ÇÅ/L‚ÇÇ, parall√©lisation, `colsample_bytree` |
| **LightGBM** | histogrammes, croissance feuille-par-feuille |
| **CatBoost** | encodage cat√©goriel natif |

---

## 9. Big Data & ML scalable

| Probl√®me | Solution / Approche | Exemple |
|-----------|---------------------|----------|
| Donn√©es trop volumineuses pour la RAM | **Apprentissage incr√©mental** | `partial_fit()` (SGDClassifier) |
| Donn√©es distribu√©es | **MapReduce / Spark MLlib** | entra√Ænement distribu√© |
| Donn√©es h√©t√©rog√®nes | **Feature hashing, encodage distribu√©** | NLP, logs |
| Mod√®le trop lent √† tuner | **RandomizedSearch / Hyperband** | tuning adaptatif |
| Pipeline complet | **MLflow, Airflow, Dataiku, scikit-pipeline** | MLOps |

---

## 10. R√©sum√© formules cl√©s

| Concept | Formule / Expression | Interpr√©tation |
|----------|---------------------|----------------|
| **Hyperplan SVM** | $w^\top x + b = 0$ | fronti√®re de d√©cision |
| **Marge** | $\frac{2}{\|w\|}$ | distance entre hyperplans supports |
| **Dual SVM** | $\max_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_{ij}\alpha_i\alpha_j y_i y_j K(x_i,x_j)$ | optimisation sur les $\alpha$ |
| **D√©cision** | $f(x)=\text{sign}\left(\sum_i \alpha_i y_i K(x_i,x) + b\right)$ | classification |
| **RBF kernel** | $\exp(-\gamma\|x-x'\|^2)$ | projection implicite |
| **Gradient boosting** | $F_m(x)=F_{m-1}(x)+\nu h_m(x)$ | apprentissage r√©siduel |
| **AdaBoost poids** | $w_i \leftarrow w_i e^{-\alpha_t y_i h_t(x_i)}$ | r√©-pond√©ration des erreurs |
| **AUC ROC** | $AUC = \int_0^1 TPR(FPR)\,dFPR$ | pouvoir discriminant |
| **Biais‚ÄìVariance** | $E[(y-\hat{f})^2] = (\text{biais})^2 + \text{variance} + \text{bruit}$ | compromis apprentissage |

---

**Exemples de questions typiques d'examen**  
*(et angle de r√©ponse rapide)*  

| Sujet | Question possible | R√©ponse courte attendue |
|-------|--------------------|--------------------------|
| SVM | "Donner la forme du probl√®me primal et du dual" | Primal : minimiser ¬Ω‚Äñw‚Äñ¬≤ + C‚àëŒæ·µ¢ ; Dual : maximiser Œ£Œ±·µ¢ ‚Äì ¬ΩŒ£Œ£Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºK(x·µ¢,x‚±º) |
| Boosting | "Quelle diff√©rence entre AdaBoost et Gradient Boosting ?" | AdaBoost : pond√®re les erreurs ; Gradient Boosting : apprend les r√©sidus. |
| Kernel | "Pourquoi utiliser un noyau ?" | Pour capturer la non-lin√©arit√© sans calculer explicitement la projection œÜ(x). |
| Th√©orie | "Qu‚Äôest-ce qu‚Äôune surrogate loss ?" | Une fonction convexe qui approxime la 0-1 loss pour rendre l‚Äôoptimisation d√©rivable. |
| Big Data | "Citer une m√©thode d‚Äôapprentissage scalable" | SGD incr√©mental / Spark MLlib / mini-batch learning. |

---

## Annexes - Mini-d√©rivations

### A. SVM lin√©aire ‚Üí probl√®me dual

1. **√âtape 1 - probl√®me primal** : On cherche l‚Äôhyperplan s√©parateur le plus large : $\min_{w,b} \frac{1}{2}\|w\|^2 \quad \text{s.c. } y_i(w^\top x_i + b) \ge 1$

2. **√âtape 2 - Lagrangien** : On introduit les multiplicateurs de Lagrange $\alpha_i \ge 0$ : $\mathcal{L}(w,b,\alpha) = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i[y_i(w^\top x_i + b) - 1]$

3. **√âtape 3 - conditions de Karush-Kuhn-Tucker (KKT)** :

$$
\begin{cases}
\frac{\partial \mathcal{L}}{\partial w} = 0 \Rightarrow w = \sum_i \alpha_i y_i x_i \\
\frac{\partial \mathcal{L}}{\partial b} = 0 \Rightarrow \sum_i \alpha_i y_i = 0
\end{cases}
$$

4. **√âtape 4 - remplacer dans le Lagrangien** :
    - $\Rightarrow \mathcal{L}(\alpha) = \sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j (x_i^\top x_j)$
    - ‚Üí **Dual du SVM lin√©aire** : $\max_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j (x_i^\top x_j)
\quad \text{s.c. } \alpha_i \ge 0, \; \sum_i \alpha_i y_i = 0$

5. **√âtape 5 - fonction de d√©cision** :
    - $f(x) = \text{sign} \left(\sum_i \alpha_i y_i (x_i^\top x) + b \right)$
    - Les $\alpha_i > 0$ sont les **vecteurs de support**.

---

### B. Passage au noyau (Kernel Trick)

1. On remplace le produit scalaire $x_i^\top x_j$ par un **noyau** $K(x_i,x_j)$ : $K(x_i,x_j) = \phi(x_i)^\top \phi(x_j)$
2. ‚Üí Nouveau dual : $\max_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j K(x_i, x_j)$
3. ‚Üí D√©cision : $f(x) = \text{sign} \left(\sum_i \alpha_i y_i K(x_i, x) + b\right)$

---

### C. Gradient Boosting ‚Äì d√©rivation conceptuelle

1. **√âtape 1 - objectif g√©n√©ral** : On cherche √† minimiser la perte empirique : $J(F) = \sum_{i=1}^n L(y_i, F(x_i))$, avec $F(x)$ = mod√®le global (somme des arbres pr√©c√©dents).

2. **√âtape 2 - descente de gradient fonctionnelle** :
    - On approxime la descente de gradient non pas sur des **param√®tres**, mais sur des **fonctions** : $r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F=F_{m-1}}$
    - Ces $r_{im}$ sont les **pseudo-r√©sidus**.

3. **√âtape 3 - apprentissage d‚Äôun mod√®le sur les r√©sidus** : On entra√Æne un mod√®le $h_m(x)$ tel que : $h_m(x_i) \approx r_{im}$

4. **√âtape 4 - mise √† jour du mod√®le global** : $F_m(x) = F_{m-1}(x) + \nu h_m(x)$, o√π $\nu \in (0,1]$ est le *learning rate*.
      ‚Üí Chaque nouveau mod√®le apprend √† **corriger les erreurs du pr√©c√©dent**.

---

### D. AdaBoost ‚Äì d√©rivation simplifi√©e

1. **√âtape 1 - pond√©ration initiale** : $w_i^{(1)} = \frac{1}{n}$
2. **√âtape 2 - erreur pond√©r√©e du classifieur $h_t$** : $\varepsilon_t = \sum_i w_i^{(t)} \mathbb{1}(h_t(x_i) \neq y_i)$
3. **√âtape 3 - pond√©ration du mod√®le** : $\alpha_t = \frac{1}{2}\ln\frac{1-\varepsilon_t}{\varepsilon_t}$
4. **√âtape 4 - mise √† jour des poids** : $w_i^{(t+1)} = w_i^{(t)} e^{-\alpha_t y_i h_t(x_i)}$  puis normalisation : $\sum_i w_i^{(t+1)} = 1$
5. **√âtape 5 - combinaison finale** : $F(x) = \text{sign} \left(\sum_t \alpha_t h_t(x)\right)$

---

### E. Dualit√©, KKT et interpr√©tation g√©om√©trique

| √âl√©ment | Interpr√©tation intuitive |
|----------|--------------------------|
| **Dualit√©** | Permet d‚Äôexprimer le probl√®me uniquement en fonction des produits scalaires (utile pour le kernel trick). |
| **KKT** | Conditions n√©cessaires d‚Äôoptimalit√© reliant primal et dual. |
| **Vecteurs de support** | Points sur la marge : ils seuls influencent la position de l‚Äôhyperplan. |
| **Marge large** | Maximiser la distance aux classes ‚Üí meilleure g√©n√©ralisation (biais ‚Üò, variance ‚Üò). |

---

### F. Liens entre AdaBoost, Gradient Boosting et SVM

| Point commun | Explication |
|---------------|-------------|
| **Marge large** | AdaBoost et SVM cherchent √† maximiser une marge moyenne. |
| **Loss convexe** | Hinge loss (SVM) ‚Üî Exponential loss (AdaBoost). |
| **It√©ratif / S√©quentiel** | Boosting ajoute des classifieurs faibles, SVM ajuste un hyperplan. |
| **Gradient view** | Boosting ‚âà descente de gradient fonctionnelle. |

---

### G. Exemples de calculs rapides (type examen)

#### a. Calculer le nombre de vecteurs support
Si le dual donne 20 $\alpha_i > 0$ sur 200 points, alors la **marge** d√©pend uniquement de ces 20 points.

#### b. Produit scalaire et marge

$\text{Marge} = \frac{2}{\|w\|} = \frac{2}{\sqrt{w_1^2 + w_2^2 + \dots + w_d^2}}$

##### i. Produit scalaire dans un noyau RBF (gaussien)

$K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$  

Exemple : $x_i=(1,2)$, $x_j=(3,1)$, $\gamma=0.5$  
‚Üí $\|x_i-x_j\|^2 = (1-3)^2+(2-1)^2=5$  
‚Üí $K= e^{-0.5√ó5} = e^{-2.5} ‚âà 0.082$

##### ii. Marge d'un SVM lin√©aire

$\text{Marge} = \frac{2}{\|w\|} = \frac{2}{\sqrt{w_1^2 + w_2^2 + \dots + w_d^2}}$  

Exemple :  $w=(2,1)$ ‚Üí ‚Üí $\|w\| = \sqrt{5}$ ‚Üí marge $= 2/\sqrt{5} \approx 0.894$

##### iii. Fonction de d√©cision du SVM

$f(x) = \text{sign} \left(\sum_i \alpha_i y_i K(x_i, x) + b \right)$  

Exemple :  
3 vecteurs support avec $\alpha y = [0.5, -0.3, 0.2]$, $K(x_i,x)=[1,0.5,0.1]$, $b=0.1$  
‚Üí $f(x)=\text{sign}(0.5√ó1 -0.3√ó0.5 + 0.2√ó0.1 + 0.1)=\text{sign}(0.47)=+1$  


#### b. Exemple de kernel polynomial

$$
K(x,x') = (x^\top x' + 1)^2 = (x_1x'_1 + x_2x'_2 + 1)^2
$$

‚Üí expansion = $x_1^2x_1'^2 + 2x_1x_2x'_1x'_2 + \dots$ (introduit automatiquement des interactions).

#### c. V√©rifier la validit√© d‚Äôun noyau
- Matrice $K$ sym√©trique  
- $\forall v$, $v^\top K v \ge 0$

#### 7.5. Losses usuelles (√† savoir √©crire)
| Nom | Expression | D√©riv√©e |
|------|-------------|----------|
| Hinge | $\max(0, 1 - yf(x))$ | $-y$ si $yf(x)<1$, 0 sinon |
| Log loss | $\log(1 + e^{-y f(x)})$ | $-\frac{y}{1+e^{y f(x)}}$ |
| MSE | $(y-f(x))^2$ | $-2(y-f(x))$ |

---

### H. Comprendre le probl√®me **primal vs dual** (ELI5)

#### a. Contexte
Un SVM cherche une **fronti√®re** (droite, plan ou hyperplan) qui s√©pare au mieux deux classes.
Cette fronti√®re est d√©finie par $w$ et $b$ dans l‚Äô√©quation $w^\top x + b = 0$.

#### i. Le probl√®me primal
C‚Äôest la **formulation directe** : "Je veux trouver le meilleur $w$ et $b$ pour maximiser la marge entre les classes."

$$
\min_{w,b} \frac{1}{2}\|w\|^2
\quad \text{s.c.} \quad y_i(w^\top x_i + b) \ge 1
$$

- $w$ = vecteur normal √† la fronti√®re  
- $b$ = position de la fronti√®re  
- Objectif = avoir une marge grande (petite norme de $w$)


#### ii. Le probl√®me dual
C‚Äôest la **formulation invers√©e** : Au lieu d‚Äôoptimiser directement $w$ et $b$, on donne √† chaque point $x_i$ un **poids** $\alpha_i$ qui indique "combien ce point pousse sur la fronti√®re".

$$
\max_{\alpha_i \ge 0} 
\sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j (x_i^\top x_j)
$$

Seuls les points **sur la marge** (les ‚Äúpoints charni√®res‚Äù) ont $\alpha_i > 0$ :
ce sont les **vecteurs de support**.

#### b. Pourquoi on s‚Äôen sert
- Le probl√®me dual **d√©pend uniquement des produits scalaires** $(x_i^\top x_j)$.
- On peut donc les **remplacer par un noyau** $K(x_i,x_j)$ pour g√©rer la non-lin√©arit√© : $f(x) = \text{sign}\left(\sum_i \alpha_i y_i K(x_i, x) + b\right)$.
- Le SVM devient ainsi **non lin√©aire sans jamais calculer la projection**.

**R√©sum√© simple :**
| Vue | Ce qu‚Äôon fait | Image mentale |
|------|---------------|---------------|
| **Primal** | On cherche directement la droite s√©paratrice $w,b$ | "Je trace la meilleure fronti√®re possible." |
| **Dual** | On exprime le probl√®me en termes d‚Äôinfluences $\alpha_i$ | "Chaque point tire plus ou moins fort sur la fronti√®re." |
| **Kernel** | On remplace les produits scalaires par un noyau $K$ | "Je tords l‚Äôespace pour s√©parer les points sans effort." |

---

## Formulaire express

### A. G√©n√©ralit√©s
- **Produit scalaire** : $x_i^\top x_j = \sum_k x_{ik} x_{jk}$
- **Distance euclidienne** : $\|x_i - x_j\| = \sqrt{\sum_k (x_{ik} - x_{jk})^2}$
- **Norme** : $\|w\| = \sqrt{w_1^2 + \dots + w_d^2}$  
- **Marge du SVM** : $\displaystyle \text{Marge} = \frac{2}{\|w\|}$  
- **R√©gularisation** :
  - L1 (Lasso) ‚Üí $\|w\|_1 = \sum_i |w_i|$
  - L2 (Ridge) ‚Üí $\|w\|_2^2 = \sum_i w_i^2$

### B. SVM lin√©aire

#### D. Probl√®me primal :
$min_{w,b} \frac{1}{2}\|w\|^2 \quad \text{s.c. } y_i(w^\top x_i + b) \ge 1$

#### ii. Lagrangien :
$\mathcal{L}(w,b,\alpha) = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i[y_i(w^\top x_i + b) - 1]$

#### iii. Conditions KKT :
$\frac{\partial \mathcal{L}}{\partial w} = 0 \Rightarrow w = \sum_i \alpha_i y_i x_i$  

$\frac{\partial \mathcal{L}}{\partial b} = 0 \Rightarrow \sum_i \alpha_i y_i = 0$  

#### ii. Probl√®me dual :
$\max_\alpha \sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j (x_i^\top x_j)$  

#### iii.  Fonction de d√©cision :
$f(x) = \text{sign} \left(\sum_i \alpha_i y_i (x_i^\top x) + b \right)$  

### C. Kernel Trick (SVM non lin√©aire)

Remplacer $x_i^\top x_j$ par $K(x_i, x_j)$ : $K(x_i, x_j) = \phi(x_i)^\top \phi(x_j)  

#### i. Exemples : 
| Noyau | Formule | Hyperparam√®tres |
|-------|----------|-----------------|
| Lin√©aire | $K(x,x') = x^\top x'$ | aucun |
| Polynomial | $K(x,x') = (x^\top x' + c)^d$ | $c$, $d$ |
| RBF (gaussien) | $K(x,x') = \exp(-\gamma \|x-x'\|^2)$ | $\gamma$ |
| Sigmo√Øde | $K(x,x') = \tanh(\beta x^\top x' + \theta)$ | $\beta$, $\theta$ |

#### ii. D√©cision finale :
$f(x) = \text{sign} \left(\sum_i \alpha_i y_i K(x_i, x) + b \right)$

### D. Boosting

#### i. AdaBoost :
1. Poids initiaux : $w_i^{(1)} = \frac{1}{n}$
2. Erreur pond√©r√©e : $\varepsilon_t = \sum_i w_i^{(t)} \mathbb{1}(h_t(x_i) \neq y_i)$
3. Poids du mod√®le : $\alpha_t = \frac{1}{2}\ln\frac{1-\varepsilon_t}{\varepsilon_t}$
4. Mise √† jour : $w_i^{(t+1)} = w_i^{(t)} e^{-\alpha_t y_i h_t(x_i)}$
5. Mod√®le final : $F(x) = \text{sign} \left(\sum_t \alpha_t h_t(x) \right)$

#### ii. Gradient Boosting :
1. Objectif : $\displaystyle J(F) = \sum_i L(y_i, F(x_i))$
2. Pseudo-r√©sidus : $r_{im} = -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}$
3. Apprentissage : $h_m(x) \approx r_{im}$
4. Mise √† jour : $F_m(x) = F_{m-1}(x) + \nu h_m(x)$

### E. Loss functions

| Nom | Formule | D√©riv√©e |
|------|----------|----------|
| Hinge | $\max(0, 1 - y f(x))$ | $-y$ si $y f(x)<1$, 0 sinon |
| Logistique | $\log(1 + e^{-y f(x)})$ | $-\frac{y}{1 + e^{y f(x)}}$ |
| Exponentielle | $e^{-y f(x)}$ | $-y e^{-y f(x)}$ |
| MSE | $(y-f(x))^2$ | $-2(y-f(x))$ |
| MAE | $|y - f(x)|$ | $\text{sign} (f(x)-y)$ |

### F. M√©triques de performance

#### i. Classification :
- $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$
- $\text{Precision} = \frac{TP}{TP + FP} \quad \text{Recall} = \frac{TP}{TP + FN}$
- $F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$
- $AUC = \int_0^1 TPR(FPR) \, d(FPR)$

#### ii. R√©gression :
- $MSE = \frac{1}{n}\sum_i (y_i - \hat{y}_i)^2$
- $RMSE = \sqrt{MSE}$
- $MAE = \frac{1}{n}\sum_i \ vert y_i - \hat{y}_i \vert $
- $R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2}$

### G. Rappels pratiques

| Cas | Mod√®le recommand√© | Hyperparam√®tres cl√©s |
|-----|-------------------|----------------------|
| Fronti√®re lin√©aire | SVM lin√©aire, r√©gression logistique | $C$ |
| Fronti√®re courbe | SVM RBF, boosting | $C$, $\gamma$ |
| Dataset petit et bruit√© | Ridge, Lasso, AdaBoost | $\lambda$, $\alpha$ |
| Dataset volumineux | RandomForest, XGBoost | `n_estimators`, `max_depth` |
| Classes d√©s√©quilibr√©es | Boosting, SVM + `class_weight` | `learning_rate`, `class_weight` |

### g. Interpr√©tation des mod√®les
- **SVM** ‚Üí marge large, points support, robustesse.  
- **AdaBoost** ‚Üí pond√®re les erreurs, renforce la marge moyenne.  
- **Gradient Boosting** ‚Üí apprend sur les r√©sidus (descente de gradient fonctionnelle).  
- **R√©gularisation** ‚Üí contr√¥le la complexit√©, r√©duit la variance.  
- **Dualit√©** ‚Üí exprime le probl√®me en fonction des produits scalaires, utile pour les noyaux.

### H. Conditions KKT (rappel rapide)

$$
\begin{cases}
\alpha_i \ge 0 \\
y_i(w^\top x_i + b) - 1 \ge 0 \\
\alpha_i[y_i(w^\top x_i + b) - 1] = 0
\end{cases}
$$

---

## R√®gles rapides √† retenir

| Concept | R√®gle |
|----------|-------|
| Donn√©es fortement corr√©l√©es | ‚Üí instabilit√© du mod√®le lin√©aire |
| Variance des r√©sidus non constante | ‚Üí h√©t√©rosc√©dasticit√© |
| R√©sidus centr√©s et al√©atoires | ‚Üí mod√®le bien sp√©cifi√© |
| AUC ‚âà 0.5 | ‚Üí mod√®le al√©atoire |
| AUC > 0.9 | ‚Üí tr√®s bon mod√®le |
| Small $C$ (SVM) | ‚Üí marge large (r√©gularisation forte) |
| Large $C$ | ‚Üí marge fine, risque de surfit |
| Small $\gamma$ (RBF) | ‚Üí fronti√®re lisse |
| Large $\gamma$ | ‚Üí fronti√®re tortueuse |

**Derni√®re minute**
- Lire les contraintes du primal ‚Üí penser "droite + marge".  
- Lire le dual ‚Üí penser "combinaison de points".  
- Produit scalaire ‚Üí penser "mesure de similarit√©".  
- Hinge loss ‚Üí marge large.  
- Boosting ‚Üí apprentissage r√©siduel.  
- Gradient Boosting ‚Üí descente de gradient sur fonctions.

---

üí° **Conseil d‚Äôexamen**

- Si l‚Äô√©nonc√© demande d"expliquer le r√¥le de la r√©gularisation", toujours mentionner :
  - elle **contr√¥le la complexit√© du mod√®le**,
  - elle **√©vite le surapprentissage**,
  - et elle **am√©liore la stabilit√©** des coefficients / marges.

- Si une question demande une *interpr√©tation g√©om√©trique*, toujours mentionner la notion de **marge** et de **vecteurs support**.

- Si c‚Äôest une question sur *les algorithmes d‚Äôensemble*, √©voquer **le compromis biais/variance** et **l‚Äôapprentissage s√©quentiel des erreurs**.</small>

---
