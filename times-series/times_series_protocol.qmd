```{yaml}
title: "Time Series Analysis: End-to-End Protocol"
author: "rsquaredata"
date: 2026-01-21
date-format: "YYYY-MM-DD"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
execute:
  echo: true
  warning: false
  message: false
```

This notebook provides an end-to-end pipeline for forecasting:

```{mermaid}
flowchart LR
    A[Data] --> B[Cleaning]
    B --> C[EDA]
    C --> D[Frequency inference]
    D --> E[Candidate models]
    E --> F[Rolling CV]
    F --> G[Model selection]
    G --> H[Refit]
    H --> I[Forecast]
    I --> J[Diagnostics]
```

# 1. Setup

```{r}
library(ggplot2)
library(ggfortify)
library(forecast)
library(fpp2)

library(readr)
library(dplyr)
library(conflicted)
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")   # optional, same idea (stats::lag vs dplyr::lag)

library(imputeTS)
library(urca)
library(patchwork)

set.seed(123)
```

# 2. Data Ingestion

## 2.1. Load data

The data can be loaded from multiple sources using a single toggle.  
Supported sources include:
-   local CSV files,
-   CSV files hosted online (e.g. GitHub raw),
-   local Excel (`.xlsx`) files,
-   Excel files (`.xlsx`) hosted online.

In all cases, the expected minimal structure is:
- `date`: Date
- `value`: numeric

```{r}
library(readxl)

# Helper for Excel (binary) files
read_xlsx_from_url <- function(url) {
  tmp <- tempfile(fileext = ".xlsx")
  download.file(url, tmp, mode = "wb", quiet = TRUE)
  read_excel(tmp)
}

# -----------------------------
# Data source toggle
# (un-comment the relevant source_type)
# -----------------------------
# source_type <- "github_csv"
# source_type <- "github_xlsx"
# source_type <- "local_csv"
# source_type <- "local_xlsx"
# source_type <- "url_xlsx"

data_raw <- switch(
  source_type,
  local_csv   = read_csv("data.csv"),
  github_csv  = read_csv(
    "https://raw.githubusercontent.com/OWNER/REPO/BRANCH/path/to/data.csv"
  ),
  local_xlsx  = read_excel("data.xlsx", sheet = "Sheet1"),
  url_xlsx    = read_xlsx_from_url("https://example.com/data.xlsx"),
  github_xlsx = read_xlsx_from_url(
    "https://raw.githubusercontent.com/OWNER/REPO/BRANCH/times/path/to/data.xlsx"
  ),
  stop("Unknown source_type")
)

# Inspect the data
head(data_raw)
str(data_raw)
```

## 2.2. Parse and sort dates

```{r}
data_raw <- data_raw |>
  mutate(
    date = case_when(
      inherits(date, "POSIXct") ~ date,
      inherits(date, "Date")    ~ as.POSIXct(date),
      is.character(date)        ~ as.POSIXct(date, tz = "UTC"),
      is.numeric(date)          ~ as.POSIXct(date, origin = "1970-01-01", tz = "UTC"),
      TRUE ~ NA_POSIXct_
    )
  ) |>
  arrange(date)

# Sanity check
stopifnot(!any(is.na(data_raw$date)))

summary(data_raw)
```

# 3. Automatic Frequency and Seasonality Strategy

## 3.1. Why frequency detection matters

For daily data, `frequency = 365` is often *theoretically* correct but **statistically suboptimal** when:
-   the dominant seasonality is **weekly**
-   the sample size is limited
-   ARIMA struggles with long seasonal periods

## 3.2. Calendar regularity check

```{r}
# Ensure date is POSIXct to preserve intraday resolution
if (!inherits(data_raw$date, "POSIXct")) {
  data_raw <- data_raw %>%
    mutate(date = as.POSIXct(date))
}

time_diffs <- diff(data_raw$date)
time_diffs_hours <- as.numeric(time_diffs, units = "hours")

summary(time_diffs_hours)
table(round(time_diffs_hours, 3))
```
Interpretation:  
-   < 1 hour → sub-hourly / intraday
-   ~1 hour → hourly
-   ~24 hours → daily
-   ~168 hours → weekly
-   ~720 hours → monthly
-   ~2160 hours → quarterly

**Interpretation**

The distribution of time differences between consecutive observations indicates that the series is sampled at a **[regular / approximately regular / irregular]** frequency.

The dominant time step is approximately **[X] hours**, corresponding to **[hourly / daily / weekly / monthly]** data.  
No major irregular gaps are observed, suggesting that the data can be modeled directly without preliminary temporal aggregation.

If irregularities were present (e.g. missing weekends or trading days), an aggregation step (weekly or monthly) would be considered prior to modeling.

## 3.3. Frequency + seasonality strategy

`ts` supports **one main seasonal frequency**. **TBATS** can model multiple seasonal periods.

For daily data, common seasonalities:
-   weekly cycle: 7
-   yearly-ish cycle: 365 (approx)

We will build a (daily-like) `ts` object, then feed it to TBATS with multiple seasonal periods when relevant.

### 3.3.1. Choose baseline frequency for TS

The baseline frequency is chosen to reflect the **dominant seasonal cycle**, not the calendar unit.

The baseline frequency is used for indexing and SARIMA only; multi-seasonality is handled exclusively by TBATS.

```{r}
# -----------------------------------------
# Automatic baseline frequency (freq_ts)
# -----------------------------------------
# freq_ts = number of observations per seasonal cycle used by ts() and SARIMA (auto.arima with seasonal=TRUE)
# NOTE:
# ts() and SARIMA support only ONE seasonal period.
# For multi-seasonality (e.g. intraday + weekly), TBATS is used later with explicit seasonal.periods.

freq_mode <- "auto"
# freq_mode <- "manual"

# Only used if freq_mode == "manual"
freq_ts_manual <- 365

# Infer median time step in days
step_days <- as.numeric(median(diff(data_raw$date)), units = "days")

if (!is.finite(step_days) || step_days <= 0) {
  stop("Cannot infer time step from `date`. Check date parsing.")
}

if (freq_mode == "auto") {

  freq_ts <- dplyr::case_when(

    # ---- Intraday ----
    step_days < 1/48 ~ round(1 / step_days),         # e.g. 15-min -> 96
    step_days < 1/24 ~ 48,                           # 30-min
    step_days < 1/12 ~ 24,                           # hourly

    # ---- Daily ----
    step_days < 2     ~ 7,                           # weekly seasonality

    # ---- Weekly ----
    step_days < 10    ~ 52,                          # yearly seasonality

    # ---- Monthly ----
    step_days < 40    ~ 12,

    # ---- Quarterly ----
    step_days < 120   ~ 4,

    # ---- Fallback ----
    TRUE              ~ 1                           # if no clear seasonality can be inferred, the series is treated
  )                                                 # as non seasonal (freq_ts = 1)

} else {
  freq_ts <- freq_ts_manual
}

message("Inferred freq_ts = ", freq_ts,
        " (step ≈ ", round(step_days, 4), " days)")

# Sanity checks
stopifnot(freq_ts >= 1)
stopifnot(is.finite(freq_ts))

if (freq_ts > length(data_raw$value) / 2) {
  warning(
    "freq_ts is large relative to series length. ",
    "Seasonal ARIMA may be unstable; TBATS is preferred."
  )
}
```
<details><summary>Reveal what the above chunk does</summary>

| Data type | Example spacing | `freq_ts` chosen | Meaning            |
|-----------|-----------------|------------------|--------------------|
| 15-min    | 0.0104 days     | 96               | daily seasonality  |
| 30-min    | 0.0208 days     | 48               | daily seasonality  |
| Hourly    | 0.0417 days     | 24               | daily seasonality  |
| Daily     | 1 day           | 7                | weekly seasonality |
| Weekly    | 7 days          | 52               | yearly seasonality |
| Monthly   | ~30 days        | 12               | yearly seasonality |
| Quarterly | ~90 days        | 4                | yearly seasonality |

</details>

**Baseline frequency choice**

Based on the inferred time step, the baseline frequency was set to **freq_ts = [value]**, corresponding to a dominant **[daily / weekly / yearly]** seasonal cycle.

This frequency is used **only for time indexing and SARIMA-type models**.  
It does not imply that the series follows a single seasonal structure.

Potential additional seasonal patterns are handled separately using multi-seasonal models (TBATS).

### 3.3.2. Build ts object

For daily indexing: start = (year, day-of-year)

```{r}
start_year <- as.numeric(format(min(data_raw$date), "%Y"))
start_sub  <- as.numeric(format(min(data_raw$date), "%j"))

ts_data <- ts(
  data_raw$value,
  start = c(start_year, start_sub),
  frequency = freq_ts
)

ts_data
```

### 3.3.3. Automatic choice of TBATS seasonal periods

```{r}
# Automatic TBATS seasonal period selection
n <- length(ts_data)   # change to length(ts_clean) after missing-value handling

# Default candidates (daily data)
tbats_candidates <- list(
  weekly = c(7),
  weekly_yearly = c(7, 365)
)

# Intraday / hourly override
# If freq_ts >= 24, freq_ts represents a daily cycle (e.g. 24, 48, 96)
if (freq_ts >= 24) {
  tbats_candidates <- list(
    daily = c(freq_ts),
    daily_weekly = c(freq_ts, freq_ts * 7)
  )
}

# Length-based constraint
if (n < 2 * max(unlist(tbats_candidates))) {
  tbats_candidates <- tbats_candidates[1]
}

tbats_candidates
```

Below is a manual method:
```{r}
# Default: daily data often has weekly + yearly seasonalities
# tbats_seasonal_periods <- c(7, 365)

# If the data are weekly (one strong annual-ish seasonality):
# tbats_seasonal_periods <- c(52)

# If the data are monthly:
# tbats_seasonal_periods <- c(12)

# If quarterly:
# tbats_seasonal_periods <- c(4)
```
*note* : if the series is short, including `365` may be too ambitious. In that case, we only try `c(7)` or `c(12)` depending on the context.

# 4. Missing values

```{r}
ggplot_na_distribution(ts_data)
ts_clean <- na_interpolation(ts_data)
sum(is.na(ts_clean))
```

# 5. Exploratory Data Analysis (EDA)

```{r}
autoplot(ts_clean) +
  labs(title = "Time series")
```
```{r}
ggseasonplot(ts_clean) +
  labs(title = "Seasonal plot (may be messy for daily series)")
```

## 5.1. Decomposition (illustrative)

This step is used for interpretation and pedagogical purposes. It does not enforce preprocessing choices in the final pipeline.

```{r}
decomp <- decompose(ts_clean)
autoplot(decomp) +
  labs(title = "Classical decomposition (exploratory)")
```

**Decomposition analysis**

The classical decomposition highlights three main components:

- **Trend**: The trend component shows a **[stable / increasing / decreasing / nonlinear]** long-term evolution.
- **Seasonality**: A recurring seasonal pattern is visible, consistent with a **[weekly / yearly / intraday]** cycle.
- **Remainder**: The remainder component appears **[low / moderate / high]** in variance, suggesting **[weak / moderate / strong]** unexplained short-term fluctuations.

This analysis is exploratory and is not used to enforce differencing or detrending decisions in the final modeling pipeline.

## 5.2. Box-Jenkins exploratory analysis (pedagogical)

The following section implements a classical Box–Jenkins analysis for exploratory and pedagogical purposes.
The resulting differenced series is not used in the final model selection, which relies on cross-validation across multiple model classes.

### Candidate differencing

```{r}
x0  <- ts_clean
x1  <- diff(ts_clean, differences = 1)
xS  <- diff(ts_clean, lag = freq_ts, differences = 1)
x1S <- diff(xS, differences = 1)
```

### Visual comparison

```{r}
autoplot(x0)  + ggtitle("Original") |
autoplot(x1)  + ggtitle("Diff(1)") |
autoplot(xS)  + ggtitle("Seasonal diff") |
autoplot(x1S) + ggtitle("Diff + seasonal diff")
```

### ACF / PACF diagnostics

```{r}
par(mfrow = c(2,2))
acf(x0,  main = "ACF original")
acf(x1,  main = "ACF diff(1)")
acf(xS,  main = "ACF seasonal diff")
acf(x1S, main = "ACF diff + seasonal")
par(mfrow = c(1,1))
```

### Optional PACF

```{r}
par(mfrow = c(2,2))
pacf(x0,  main = "PACF original")
pacf(x1,  main = "PACF diff(1)")
pacf(xS,  main = "PACF seasonal diff")
pacf(x1S, main = "PACF diff + seasonal")
par(mfrow = c(1,1))
```

**ACF / PACF interpretation**

The ACF of the original series exhibits **[slow decay / significant seasonal spikes / rapid cutoff]**, indicating the presence of **[non-stationarity / seasonality]**.

After differencing, the ACF and PACF show **[reduced autocorrelation / remaining short-term dependence]**, suggesting that **[one / both]** of the following may be required:
- non-seasonal differencing,
- seasonal differencing.

These observations are consistent with the classical Box–Jenkins identification approach and motivate the consideration of ARIMA/SARIMA-type models.

### Box-Jenkins ARIMA (illustrative, not selected globally)

```{r}
# Manual ARIMA inspired by ACF/PACF (illustrative)
fit_bj <- auto.arima(
  ts_clean,
  seasonal = TRUE,
  stepwise = FALSE,
  approximation = FALSE
)

checkresiduals(fit_bj)
cat("This ARIMA model is used for Box–Jenkins illustration only.\n")
```

**Box–Jenkins model diagnostics (illustrative)**

The residual diagnostics of the manually specified ARIMA model show:
- residuals that are **[approximately / not]** centered around zero,
- **[absence / presence]** of significant autocorrelation,
- a Ljung–Box test p-value of **[value]**.

This model is used for pedagogical illustration only.  
Final model selection is based on rolling-origin cross-validation across multiple model families.

# 6. Train/Test Split (Final Evaluation)

## 6.1. Holdout Split for Final Evaluation

We keep a final holdout test. Model selection will use rolling-origin CV inside the training set.

```{r}
h_test <- 30  # final test horizon (daily)

# weekly:  h_test <- 8
# monthly: h_test <- 12
# quarterly: h_test <- 8

train_full <- window(ts_clean, end = length(ts_clean) - h_test)
test_final <- window(ts_clean, start = length(ts_clean) - h_test + 1)

length(train_full)
length(test_final)
```

## 6.2. Exponential Smoothing Models (illustrative)

This section explicitly fits the classical exponential smoothing models (SES, Holt, Holt–Winters).

These models are included for **illustrative and pedagogical purposes**. They are **not used** for cross-validation-based model selection, which relies on broader model families (ETS, ARIMA, NNAR, TBATS).

```{r}
fit_ses  <- ses(train_full, h = h_test)
fit_holt <- holt(train_full, h = h_test)
fit_hw_add <- hw(train_full, seasonal = "additive", h = h_test)

# Optional: multiplicative if variance increases with level
# fit_hw_mul <- hw(train_full, seasonal = "multiplicative", h = h_test)
```

Quick visual comparison

```{r}
autoplot(train_full) +
  autolayer(fit_ses$mean, series = "SES") +
  autolayer(fit_holt$mean, series = "Holt") +
  autolayer(fit_hw_add$mean, series = "HW additive") +
  labs(title = "Exponential Smoothing Models (Illustrative)")
```

**Exponential smoothing models**

The SES, Holt, and Holt–Winters models illustrate different assumptions regarding trend and seasonality:

- SES assumes no trend and no seasonality,
- Holt allows for a trend,
- Holt–Winters incorporates both trend and seasonality.

Visual inspection suggests that **[model name]** better captures the observed dynamics.  
However, these models are included for illustration only and are not used in the final model selection process.

# 7. Rolling-Origin Cross-Validation Utilities

We will compare models using time-series CV:  
-   expanding window
-   forecasting `h_cv` steps ahead
-   compute RMSE over all folds

## RMSE

```{r}
rmse <- function(y_true, y_pred) {
  sqrt(mean((y_true - y_pred)^2, na.rm = TRUE))
}
```

## Make folds

```{r}
# Create rolling-origin folds
# initial: initial training size
# h_cv: forecasting horizon per fold
# step: shift between folds
make_folds <- function(x, initial, h_cv, step) {
  n <- length(x)
  stops <- seq(initial, n - h_cv, by = step)
  lapply(stops, function(stop_idx) {
    list(
      train = window(x, end = stop_idx),
      test  = window(x, start = stop_idx + 1, end = stop_idx + h_cv)
    )
  })
}
```

## Choose CV parameters

```{r}
# CV horizon: small (daily) so we can repeat many folds
h_cv <- 14

# For weekly/monthly, typical:
# weekly:  h_cv <- 4
# monthly: h_cv <- 6

# initial training size:
initial <- max(100, round(0.6 * length(train_full)))

# step between folds:
step <- max(1, round(0.05 * length(train_full)))

folds <- make_folds(train_full, initial = initial, h_cv = h_cv, step = step)
length(folds)
```

## TBATS seasonal-period selection via CV (Conditional)

```{r}
tbats_cv_scores <- sapply(tbats_candidates, function(sp) {
  cv_score_model("tbats", folds, seasonal_periods = sp)
})

tbats_cv_scores

tbats_seasonal_periods <- tbats_candidates[[which.min(tbats_cv_scores)]]
tbats_seasonal_periods
```

## Speed Mode (Optional)

TBATS models are computationally expensive, especially when combined with rolling-origin cross-validation and multiple seasonalities.

Speed mode provides strategies to reduce computation time while preserving statistical validity. It is recommended for exploratory analysis, long daily series, or interactive notebooks.

| **When to use speed mode**  | **When not to use speed mode** |
|-----------------------------|--------------------------------|
| • exploratory analysis</br> • long daily series (> 5–10 years)</br> • interactive experimentation | • final report</br> • grading or evaluation</br> • production model selection |

### Option 1 - Fewer folds for TBATS

```{r}
# Speed mode option 1: subsample folds for TBATS only
folds_tbats <- folds[seq(1, length(folds), by = 2)]
```
Use `folds_tbats`**only** when scoring TBATS.

### Option 2 - Conditional activation (seasonality check)

```{r}
acf_vals <- acf(ts_clean, plot = FALSE)$acf
has_strong_seasonality <- max(abs(acf_vals[-1])) > 0.3

if (!has_strong_seasonality) {
  model_names <- setdiff(model_names, "tbats")
}
```

### Option 3 - Series-length threshold

```{r}
if (length(ts_clean) > 5000) {
  model_names <- setdiff(model_names, "tbats")
}
```

# 8. Candidate Models

We include:
-   Baselines: naive, seasonal naive, mean
-   ETS
-   ARIMA
-   NNAR
-   TBATS (multi-seasonality)

## 8.1. Model Fit + Forecast Wrapper

```{r}
fit_and_forecast <- function(model_name, train, h, seasonal_periods = c(7,365)) {
  if (model_name == "naive") {
    return(naive(train, h = h))
  }
  if (model_name == "snaive") {
    return(snaive(train, h = h))
  }
  if (model_name == "meanf") {
    return(meanf(train, h = h))
  }
  if (model_name == "ets") {
    fit <- ets(train)
    return(forecast(fit, h = h))
  }
  if (model_name == "arima") {
    fit <- auto.arima(train, seasonal = TRUE, stepwise = TRUE, approximation = TRUE)
    return(forecast(fit, h = h))
  }
  if (model_name == "nnar") {
    fit <- nnetar(train)
    return(forecast(fit, h = h))
  }
  if (model_name == "tbats") {
    fit <- tbats(train, seasonal.periods = seasonal_periods)
    return(forecast(fit, h = h))
  }
  stop("Unknown model_name")
}
```

## 8.2. Cross-Validation Scoring

```{r}
model_names <- c("naive", "snaive", "meanf", "ets", "arima", "nnar", "tbats")

cv_score_model <- function(model_name, folds, seasonal_periods) {
  fold_rmses <- sapply(folds, function(fold) {
    fc <- fit_and_forecast(model_name, fold$train, h = length(fold$test), seasonal_periods)
    rmse(as.numeric(fold$test), as.numeric(fc$mean))
  })
  mean(fold_rmses, na.rm = TRUE)
}

cv_scores <- sapply(model_names, function(m) cv_score_model(m, folds, tbats_seasonal_periods))
cv_scores
```

```{r}
cv_table <- data.frame(
  model = model_names,
  cv_rmse = as.numeric(cv_scores)
) %>% arrange(cv_rmse)

cv_table

best_model_cv <- cv_table$model[1]
best_model_cv
```

**Cross-validation results**

Rolling-origin cross-validation indicates that the **[model name]** model achieves the lowest average RMSE across folds.

This suggests that the model provides the best compromise between bias and variance on unseen data, according to the chosen evaluation metric.

# 9. Final Evaluation on Holdout Test

Now we fit the selected model on `train_full` and evaluate on `test_final`.

```{r}
fc_best <- fit_and_forecast(best_model_cv, train_full, h = length(test_final), tbats_seasonal_periods)
accuracy(fc_best, test_final)
```

We also compare to baselines on final test (sanity check):

```{r}
acc_final <- rbind(
  naive  = accuracy(naive(train_full,  h = length(test_final)), test_final)[2,],
  snaive = accuracy(snaive(train_full, h = length(test_final)), test_final)[2,],
  meanf  = accuracy(meanf(train_full,  h = length(test_final)), test_final)[2,],
  best   = accuracy(fc_best, test_final)[2,]
)

acc_final
```

## Holdout Comparison Table

```{r}
acc_holdout <- rbind(
  naive = accuracy(naive(train_full, h = h_test), test_final)[2,],
  snaive = accuracy(snaive(train_full, h = h_test), test_final)[2,],
  meanf = accuracy(meanf(train_full, h = h_test), test_final)[2,],
  ses = accuracy(fit_ses, test_final)[2,],
  holt = accuracy(fit_holt, test_final)[2,],
  hw = accuracy(fit_hw_add, test_final)[2,],
  ets = accuracy(forecast(ets(train_full), h = h_test), test_final)[2,],
  arima = accuracy(forecast(auto.arima(train_full), h = h_test), test_final)[2,],
  nnar = accuracy(forecast(nnetar(train_full), h = h_test), test_final)[2,],
  tbats = accuracy(
    forecast(tbats(train_full, seasonal.periods = tbats_seasonal_periods),
             h = h_test),
    test_final
  )[2,]
)

acc_holdout
```

**Holdout evaluation**

The performance on the holdout set confirms that the selected model generalizes well beyond the training period.

Compared to baseline models, the selected model achieves a **[lower / comparable]** error, validating the cross-validation-based selection strategy.

# 10. Refit on Full Data + Forecast for The Future

Forecast horizon for production:

```{r}
h_future <- 30
# weekly:  h_future <- 8
# monthly: h_future <- 12
```

```{r}
final_forecast <- fit_and_forecast(best_model_cv, ts_clean, h = h_future, tbats_seasonal_periods)
final_forecast
```

# 11. Final Forecast Plot

```{r}
autoplot(final_forecast) +
  labs(
    title = "Final Forecast (V3)",
    subtitle = paste("Selected by CV:", best_model_cv,
                     "| TBATS seasonal periods:", paste(tbats_seasonal_periods, collapse = ", "))
  )
```

# 12. Diagnostics

If the selected model is ARIMA/ETS/NNAR/TBATS, `checkresiduals()` works on the fitted model object.  
But here we have a forecast object, so we refit the best model to inspect residuals properly.

```{r}
refit_best_model <- function(model_name, x, seasonal_periods) {
  if (model_name == "ets") return(ets(x))
  if (model_name == "arima") return(auto.arima(x, seasonal = TRUE))
  if (model_name == "nnar") return(nnetar(x))
  if (model_name == "tbats") return(tbats(x, seasonal.periods = seasonal_periods))
  return(NULL)  # baselines don't have meaningful residual diagnostics
}

best_fit_obj <- refit_best_model(best_model_cv, train_full, tbats_seasonal_periods)

if (!is.null(best_fit_obj)) {
  checkresiduals(best_fit_obj)
} else {
  cat("Selected model is a baseline; residual diagnostics not applicable.\n")
}
```

**Residual diagnostics**

The residual diagnostics of the final model indicate that:
-   residuals are **[approximately / not]** white noise,
-   no significant autocorrelation remains,
-   the Ljung–Box test yields a p-value of **[value]**.

As the p-value exceeds the 5% significance level, we fail to reject the null hypothesis of uncorrelated residuals.  
This suggests that the model adequately captures the temporal structure of the data.

# 13. Methodology summary

This notebook implements a complete and reproducible workflow for time series forecasting, combining classical statistical theory with modern model selection practices.

The methodology follows these main steps:

1.	Data preparation

The raw data is parsed, sorted chronologically, and converted into a ts object. Calendar regularity is checked to ensure compatibility with time series models.

2.	Frequency and seasonality strategy

A baseline frequency is selected for indexing purposes, while potential seasonal structures are identified. For daily data, both weekly and yearly seasonalities are considered. When relevant, multi-seasonal models (TBATS) are used.

3.	Exploratory analysis

Visualization, seasonal plots, and classical decomposition are used to understand the structure of the series. A Box–Jenkins analysis (differencing, ACF/PACF) is included for pedagogical and interpretative purposes only.

4.	Illustrative classical models

Classical exponential smoothing models (SES, Holt, Holt–Winters) are fitted explicitly to illustrate the concepts taught in the course. These models are not used for final model selection.

5.	Holdout split

A final portion of the series (holdout set) is reserved and excluded from all model selection steps. This set is used only for the final evaluation of the selected model.

6.	Rolling-origin cross-validation

Model selection is performed using an expanding-window cross-validation scheme. Candidate models include baselines, ETS, ARIMA/SARIMA, NNAR, and TBATS. Performance is measured using RMSE averaged across folds.

7.	Model selection and refitting

The model with the best cross-validated performance is selected and refitted on the full training data.

8.	Final evaluation and forecasting

The selected model is evaluated on the holdout set, then refitted on the complete dataset to produce future forecasts.

9.	Diagnostics
Residual diagnostics are performed on the fitted model to assess remaining autocorrelation and model adequacy.

This approach ensures a clear separation between exploratory analysis, model selection, and final evaluation, while remaining consistent with both classical time series theory and modern validation practices.







